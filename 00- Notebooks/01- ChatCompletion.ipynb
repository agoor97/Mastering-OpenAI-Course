{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to `Building End-to-End Systems using OpenAI API | Arabic` course\n",
    "* This course will take you in a trip to master APIs of OpenAI (GPT, DALL-E, Whisper).\n",
    "* We will build end-to-end systems using OpenAI APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `API` (Application Programmin Interface):\n",
    "* An application programming interface is a way for two or more computer programs to communicate with each other. \n",
    "* It is a type of software interface, offering a service to other pieces of software."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dive into OpenAI`\n",
    "* Firstly, Create an accoung on OpenAI - [OpenAI_SignUp](https://platform.openai.com/signup)\n",
    "* Create a secret key, and keep it in a safe place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the main Libraries\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Key token\n",
    "_ = load_dotenv()\n",
    "key_token = os.getenv('OpenAI_KEY_TOKEN')\n",
    "\n",
    "## Assign that key_token to api_key of OpenAI\n",
    "openai.api_key = key_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Checking Models in OpenAI platform`\n",
    "#### `Checking Pricing for each service`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Text Completion Model (Completion API)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AI for a smarter tomorrow\n"
     ]
    }
   ],
   "source": [
    "## Call a model for completion\n",
    "\n",
    "user_prompt = 'Write a slogan for AI company'\n",
    "response = openai.Completion.create(\n",
    "                model='text-davinci-002',\n",
    "                prompt=user_prompt,\n",
    "                temperature=0.7,  ## How much randomness in the output (not deterministic)\n",
    "                max_tokens=1000   ## Max tokens of sum to input and output. (max_tokens of that model is 4097 token.)\n",
    "            )\n",
    "\n",
    "## Temperature (0 > 2), 0: the model is very deterministic (less randomness, more specified).\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Connecting ChatGPT (Chat Completion API)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'Do you know Geoffery Hinton ?'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'Answer as concisely as possible'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              ## recommend to use (gpt-3.5-turbo) over the other GPT-3.5 models because of its lower cost, with highest quality. \n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0.7,  \n",
    "              ## tempreature --> default=1, ranges (0 > 2) .. 0: the model will be deterministic, 2: the model has variability\n",
    "              max_tokens=1000,   ## The max_tokens (sum of input and output prompts)\n",
    "                )\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am an AI language model and I know who Geoffrey Hinton is. Geoffrey Hinton is a prominent computer scientist and cognitive psychologist who is considered one of the pioneers of deep learning and artificial neural networks. He is a professor at the University of Toronto and a researcher at Google Brain. Hinton is a recipient of numerous awards including the Turing Award, which is considered the Nobel Prize of computing. He has made significant contributions to the field of machine learning, including the development of the backpropagation algorithm, which is widely used in training artificial neural networks.\n"
     ]
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'Do you know Geoffery Hinton ?'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'Answer as detailed as possible'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              ## recommend to use (gpt-3.5-turbo) over the other GPT-3.5 models because of its lower cost, with highest quality. \n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0.7,  \n",
    "              ## tempreature --> default=1, ranges (0 > 2) .. 0: the model will be deterministic, 2: the model has variability\n",
    "              max_tokens=1000,   ## The max_tokens (sum of input and output prompts)\n",
    "                )\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'Do you know Geoffery Hinton ?'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'You are an AI Assistant, Answer questions in the shortest way'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              ## recommend to use (gpt-3.5-turbo) over the other GPT-3.5 models because of its lower cost, with highest quality. \n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0.7,  \n",
    "              ## tempreature --> default=1, ranges (0 > 2) .. 0: the model will be deterministic, 2: the model has variability\n",
    "              max_tokens=1000,   ## The max_tokens (sum of input and output prompts)\n",
    "                )\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tokens`\n",
    "* Sentence is splitted to chunks of Characters each one is considered to be a Token\n",
    "* 1000 token is about 750 words (rough Approximation.), 1 Token is approximately 4 characters.\n",
    "* ChatGPT (gpt-3.5-turbo) maximum tokens is 4096 token (input and output prompts)\n",
    "* You can use [/tokenizer](https://platform.openai.com/tokenizer) for checking your count or use tiktoken library on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7PI1WOB58YZgpllISDYWvrWeEkj95 at 0x223e6c87b50> JSON: {\n",
       "  \"id\": \"chatcmpl-7PI1WOB58YZgpllISDYWvrWeEkj95\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1686261838,\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 37,\n",
       "    \"completion_tokens\": 19,\n",
       "    \"total_tokens\": 56\n",
       "  },\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"The tallest tower in the world is the Burj Khalifa in Dubai, United Arab Emirates.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'What is the tallest tower in the world ?'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'You are an AI Assistant, Answer questions as concisely as possible.'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0.7,  \n",
    "              max_tokens=1000\n",
    "              )\n",
    "## Get the all resposne -- to check the Tokens count in this request\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Parameters of Chat Completion API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mohamed Salah is an Egyptian professional football player who plays as a forward for Liverpool FC and the Egyptian national team. He is known for his incredible speed, goal-scoring abilities, and impressive dribbling skills. He has won numerous awards and accolades for his performances on the field and is widely considered one of the best players in the world.\n",
      "Mohammed Salah is an Egyptian professional footballer who plays as a forward for the English club Liverpool and the Egypt national team. He is considered one of the best football players in the world and has won numerous awards throughout his career. He is known for his speed, dribbling skills, and goal-scoring ability.\n"
     ]
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'What do you know about Mohammed Salah ?'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'You are an AI Assistant, Answer questions as concisely as possible.'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=1.5,   \n",
    "              max_tokens=1000,\n",
    "              top_p=0.9,\n",
    "              n=2,\n",
    "              )\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++ Chat Completion Parameters ++++++++++++++++++++++++++++++++++++++ ##\n",
    "## Temperature:\n",
    "# 1. Default is 1, It is between (0 --> 2).\n",
    "# 2. Inncrease it, The model will be more diverse (Randomness) and may produce some errors.\n",
    "# 3. Reducing it, The model will be more deterministic and reduce its randomness.\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ##\n",
    "## max_tokens:\n",
    "# 1. The max_tokens as mentioned earlier for both input and output prompts\n",
    "# 2. The max_tokens for gpt-3.5-turbo (ChatGPT) is 4096 tokens, and for GPT-4 is 8192 tokens\n",
    "# 3. gpt-4-32k-0314 (beta model | June 2023) is about more than 32000 tokens.\n",
    "# 4. When you use this parameter, You are controlling to not exceed this limit as the pricing is per/1k token.\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ##\n",
    "## top_p:\n",
    "# 1. It is an alternative to temperature paramter.\n",
    "# 2. It is between (0 --> 1)\n",
    "# 3. Let's say top_p=0.2, It means that the model takes into consideration the top 20% words (common words) in vocabulary.\n",
    "# 4. If top_p is = 1, It means to take into consideration all words in vocabulary.\n",
    "# 5. Use temperature or top_p, It is not recommended to use both.\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ##\n",
    "## n:\n",
    "# 1. The Defualt is 1\n",
    "# 2. It is the number of chat completion requests required to be retrieved.\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ##\n",
    "## frequency_penalty:\n",
    "# 1. The Default value is 0, Its range between (-2 --> 2)\n",
    "# 2. The higher positive value, the lower probability that the model repeat words.\n",
    "# 3. The lower negative value, The higher probability that the model repeat words and repeat itself.\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ##\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n",
    "print(response['choices'][1]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an example SQL query: \n",
      "\n",
      "```\n",
      "SELECT name, street_address, city, state \n",
      "FROM customers \n",
      "WHERE state='California' \n",
      "ORDER BY name ASC\n"
     ]
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'Write an example of SQL Query'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'You are a smart assistant'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=1.5,   \n",
    "              max_tokens=1000,\n",
    "            #   top_p=0.9,\n",
    "            #   n=1,\n",
    "            stop=[';', '*']\n",
    "              )\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nile River is one of the most iconic and important rivers in the world. It is the longest river in the world, stretching over 4,000 miles from its source in Burundi to its delta in Egypt. The Nile has played a crucial role throughout human history, serving as a vital source of water and sustenance for countless civilizations along its banks.\n",
      "\n",
      "The Nile River is formed by two main tributaries: the White Nile and the Blue Nile. The White Nile originates from Lake Victoria in East Africa and flows through Uganda, Sudan, and South Sudan before merging with the Blue Nile at Khartoum, Sudan. The Blue Nile originates from Ethiopia's Lake Tana and flows through Ethiopia before joining with the White Nile.\n",
      "\n",
      "The ancient Egyptians revered the river as a god and believed that it was responsible for their prosperity. They built elaborate temples along its banks dedicated to Hapi, their god of fertility who was depicted as a man with large breasts representing abundance. The annual flooding of the river also played an important role in Egyptian agriculture as it deposited nutrient-rich silt onto farmland.\n",
      "\n",
      "Today, millions of people still rely on the river for their livelihoods. Egypt's population centers are concentrated around the fertile land near its delta where agriculture thrives due to irrigation systems that have been developed over centuries. However, climate change threatens this way of life as rising sea levels threaten to inundate low-lying areas near Egypt's coast.\n",
      "\n",
      "In addition to being an important source of water for humans, animals such as hippos and crocodiles call it home while fish such as tilapia thrive within its waters. Many species depend on this ecosystem for survival which makes conservation efforts crucial.\n",
      "\n",
      "Despite being such an integral part of human history and modern-day life alike, there are still many mysteries surrounding The Nile River that scientists are working hard to unravel – like how it came into existence or how long ago humans first settled along its banks.\n",
      "\n",
      "In conclusion, The Nile River is a vital part of our world's ecosystem and history. It is a symbol of life and fertility that has sustained countless civilizations for thousands of years. However, it faces many challenges in the present day as climate change threatens its future. It is up to us to ensure that this iconic river remains healthy and vibrant for generations to come.\n"
     ]
    }
   ],
   "source": [
    "## Connect API\n",
    "user_prompt = 'Write an article about The Nile River'\n",
    "\n",
    "## messgaes (the roles you put for system, user, and assistant)\n",
    "system_prompt = 'you are a smart assistant.'\n",
    "messages = [ \n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "        ]\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0.5,   \n",
    "              max_tokens=1000,\n",
    "              frequency_penalty=1  ## The probability to repeat words is much lower (the defulat is 0)\n",
    "              )\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `System Prompt in much more details`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Oriented Programming (OOP) is a programming paradigm that is based on the concept of objects. An object is an instance of a class, which is a blueprint that defines the properties and behaviors of the object. OOP focuses on the organization of code into reusable, modular components that can be easily maintained and extended.\n",
      "\n",
      "In OOP, objects have attributes (data) and methods (functions) that operate on that data. The attributes represent the state of the object, while the methods represent the behavior of the object. Encapsulation is a key feature of OOP, which means that the data and methods of an object are kept together and hidden from the outside world. This helps to ensure that the object is used correctly and prevents unintended changes to its state.\n",
      "\n",
      "Inheritance is another important feature of OOP, which allows classes to inherit properties and methods from other classes. This promotes code reuse and helps to reduce duplication. Polymorphism is also a key feature of OOP, which allows objects to take on multiple forms and behave differently depending on the context in which they are used.\n",
      "\n",
      "Here is an example of a simple class in Python:\n",
      "\n",
      "```\n",
      "class Person:\n",
      "    def __init__(self, name, age):\n",
      "        self.name = name\n",
      "        self.age = age\n",
      "\n",
      "    def say_hello(self):\n",
      "        print(\"Hello, my name is\", self.name)\n",
      "\n",
      "person1 = Person(\"Alice\", 25)\n",
      "person1.say_hello() # Output: Hello, my name is Alice\n",
      "```\n",
      "\n",
      "Question: What is the difference between a class and an object in OOP?\n"
     ]
    }
   ],
   "source": [
    "## system prompt (customizing chatgpt)\n",
    "system_prompt = ''' You are an AI assistant. \n",
    "                    Instructions:\n",
    "                        1. Explain concepts in depth using simple terms.\n",
    "                        2. Give some clear examples in python.\n",
    "                        3. Finally, Ask a question to check understanding the topic.\n",
    "                '''\n",
    "\n",
    "## Your own prompt\n",
    "user_prompt = 'Explain Object Oriented Programming'\n",
    "\n",
    "## Preparing messages\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "]\n",
    "\n",
    "## Request\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0,  \n",
    "              max_tokens=1000,  \n",
    "                )\n",
    "\n",
    "## output\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programming paradigm based on the concept of \"objects\", which can contain data and code to manipulate that data.\n"
     ]
    }
   ],
   "source": [
    "## system prompt (customizing chatgpt)\n",
    "system_prompt = ''' You are an AI assistant, Your role is to explain in the concisely as possible with no details.\n",
    "                '''\n",
    "\n",
    "## Your own prompt\n",
    "user_prompt = 'Explain Object Oriented Programming'\n",
    "\n",
    "## Preparing messages\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "]\n",
    "\n",
    "## Request\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0,  \n",
    "              max_tokens=1000,  \n",
    "                )\n",
    "\n",
    "## output\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Prompt Engineering`\n",
    "\n",
    "* Prompt engineering is the process of crafting and refining the instruction or query you feed to a generative AI tool to get a specific response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أنا مهندس ذكاء اصطناعي مبتكر وعلمي.\n"
     ]
    }
   ],
   "source": [
    "# 0. These rough rules objective is to get the best response (API level not chatgpt as a service).\n",
    "# 1. Use the latest model.\n",
    "# 2. Use curly braces {} in prompt to specify something you want to work on it.\n",
    "# 3. Put the Instructions at the beginning of the prompt.\n",
    "\n",
    "system_prompt = 'You are a smart assistat'\n",
    "user_prompt = '''Translate from English to Arabic {\n",
    "              {I'm an innovative and scientifically Artificial Intelligence Engineer}\n",
    "            '''\n",
    "\n",
    "## Preparing messages\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "]\n",
    "\n",
    "## Request\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0,  \n",
    "              max_tokens=1000,  \n",
    "                )\n",
    "\n",
    "\n",
    "## output\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: OpenAI, American, artificial intelligence, research laboratory, non-profit, Incorporated, for-profit, subsidiary corporation, Limited Partnership, AI research, promoting, developing, friendly AI.\n"
     ]
    }
   ],
   "source": [
    "# 4. Make your context (input prompt) descriptive and clear.\n",
    "# 5. You can make instructions.\n",
    "# 6. Don't say what not to do, Say what to do instead.\n",
    "\n",
    "user_prompt = ''' Extract the useful keyword from the below text.\n",
    "Text: {OpenAI is an American artificial intelligence research laboratory consisting of \n",
    "        the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. \n",
    "        OpenAI conducts AI research with the declared intention of promoting and developing friendly AI.}\n",
    "        \n",
    "        Your Results should be: keywords: {Your results here}.\n",
    "'''\n",
    "\n",
    "\n",
    "## Preparing messages\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt}, \n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "]\n",
    "\n",
    "## Request\n",
    "response = openai.ChatCompletion.create(\n",
    "              model='gpt-3.5-turbo',           \n",
    "              messages=messages,\n",
    "              temperature=0,  \n",
    "              max_tokens=1000,  \n",
    "                )\n",
    "\n",
    "\n",
    "## output\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
